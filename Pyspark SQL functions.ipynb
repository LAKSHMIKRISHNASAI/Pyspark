{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc997f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cad2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22217e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"predictions\").master(\"local[2]\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71735ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-EPRO57M:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>predictions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ad9c67b750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c11f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"D://users//krishnasai//Downloads//flight_price_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40d2c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----------+--------------+-----+------------+----------------+--------+---------+-------+\n",
      "|filghtId|  airline|flight|source_city|departure_time|stops|arrival_time|destination_city|duration|days_left|  class|\n",
      "+--------+---------+------+-----------+--------------+-----+------------+----------------+--------+---------+-------+\n",
      "|  278519|Air_India|AI-559|  Hyderabad| Early_Morning|  one|       Night|          Mumbai|   14.75|       22|Economy|\n",
      "|  264230|  Vistara|UK-720|    Kolkata| Early_Morning|  one|     Evening|          Mumbai|    9.33|        6|Economy|\n",
      "|  260097|  Vistara|UK-852|  Bangalore|       Morning|  one|     Evening|         Chennai|   10.42|       27|Economy|\n",
      "|  218935|  Vistara|UK-747|      Delhi| Early_Morning| zero|     Morning|         Kolkata|    2.08|       47|Economy|\n",
      "|  211085|Air_India|AI-762|      Delhi|         Night|  one|       Night|          Mumbai|   23.83|       41|Economy|\n",
      "|  251748|Air_India|AI-505|  Bangalore|       Morning|  one|       Night|          Mumbai|   11.17|       37|Economy|\n",
      "|  292760|Air_India|AI-430|    Chennai|       Morning|  one|       Night|          Mumbai|   11.33|       39|Economy|\n",
      "|  292966|Air_India|AI-440|    Chennai| Early_Morning|  one|       Night|          Mumbai|   15.08|       42|Economy|\n",
      "|  245610|Air_India|AI-565|  Bangalore|     Afternoon|  one|     Morning|           Delhi|   18.75|       12|Economy|\n",
      "|  261821|Air_India|AI-773|    Kolkata|       Evening|  one|   Afternoon|           Delhi|    19.0|       11|Economy|\n",
      "|  283750|  Vistara|UK-830|  Hyderabad|       Morning|  one|     Evening|         Kolkata|    9.25|       18|Economy|\n",
      "|  282713|  Vistara|UK-874|  Hyderabad|       Morning|  one|       Night|       Bangalore|   12.42|       45|Economy|\n",
      "|  281982|Air_India|AI-698|  Hyderabad|         Night|  one|     Evening|       Bangalore|   20.25|       32|Economy|\n",
      "|  252526|Air_India|AI-604|  Bangalore|       Morning| zero|     Morning|          Mumbai|     2.0|       46|Economy|\n",
      "|  279317|Air_India|AI-840|  Hyderabad|         Night|  one|   Afternoon|          Mumbai|    16.0|       34|Economy|\n",
      "|  285809|  Vistara|UK-860|  Hyderabad| Early_Morning|  one|       Night|         Chennai|   13.42|       12|Economy|\n",
      "|  246698|  Vistara|UK-852|  Bangalore|       Morning|  one|     Evening|           Delhi|    8.58|       25|Economy|\n",
      "|  274732|  Vistara|UK-720|    Kolkata| Early_Morning|  one|       Night|         Chennai|   13.08|       49|Economy|\n",
      "|  271464|  Vistara|UK-738|    Kolkata|       Evening|  one|     Evening|       Hyderabad|   22.25|       27|Economy|\n",
      "|  243380|Air_India|AI-888|     Mumbai|       Evening|  one|     Evening|         Chennai|    24.0|       28|Economy|\n",
      "+--------+---------+------+-----------+--------------+-----+------------+----------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf9396c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed7910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.select(\"filghtId\",\"source_city\").groupBy(\"source_city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "236b9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK_HOME=\"Path\"\n",
    "# HADOOP_HOM=\"Path\"\n",
    "# path= %PATH%;\"path\"\\bin\n",
    "\n",
    "# kafka console producer shell\n",
    "# kafka-console-producer.sh --broker-list localhost:port --topic json_topic\n",
    "# kafka streaming with sql has two processes-\n",
    "# sql batch processing- consuming messages from kafka\n",
    "# producing data to kafka\n",
    "###'''spark.write.format(\"kafka\").outputMode(\"complete\").option(\"topic\",\"json_topic\")\n",
    "# start().awaitTermination()''' \n",
    "# val data=spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"127.0.0.1:4040\")\n",
    "# .option(\"subscribe\",\"json_topic\").load()\n",
    "d=spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e34ed991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9881605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----------+----------------+\n",
      "|filghtId|  airline|flight|source_city|destination_city|\n",
      "+--------+---------+------+-----------+----------------+\n",
      "|  251748|Air_India|AI-505|  Bangalore|          Mumbai|\n",
      "|  252526|Air_India|AI-604|  Bangalore|          Mumbai|\n",
      "|  251389|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  250349|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  252051|  Vistara|UK-816|  Bangalore|          Mumbai|\n",
      "|  250451|  Vistara|UK-818|  Bangalore|          Mumbai|\n",
      "|   98064|  Vistara|UK-812|  Bangalore|          Mumbai|\n",
      "|  249532|Air_India|AI-808|  Bangalore|          Mumbai|\n",
      "|  250307|  Vistara|UK-657|  Bangalore|          Mumbai|\n",
      "|  251483|Air_India|AI-808|  Bangalore|          Mumbai|\n",
      "|  252602|  Vistara|UK-866|  Bangalore|          Mumbai|\n",
      "|  251689|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  250260|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  249935|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  249250|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  252043|  Vistara|UK-812|  Bangalore|          Mumbai|\n",
      "|  249328|Air_India|AI-610|  Bangalore|          Mumbai|\n",
      "|  250921|  Vistara|UK-854|  Bangalore|          Mumbai|\n",
      "|  252301|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "|  249567|  Vistara|UK-810|  Bangalore|          Mumbai|\n",
      "+--------+---------+------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"filghtId\",\"airline\",\"flight\",\"source_city\",\"destination_city\").filter(data.destination_city==\"Mumbai\").filter(data.source_city==\"Bangalore\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d91d7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af23fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spark= SparkConf().setAppName(\"pyspark learning\").setMaster(\"local[1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89ff18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext.getOrCreate(new_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd62aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predictions'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81f8a436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[2]'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a016c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(range(0,20),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbb8ede8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd5cfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "09fa6185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd.partitions.size\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "95ee53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.coalesce\n",
    "rdd2= sc.parallelize(range(50),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3646246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd=rdd2.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc8a5a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74624205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=rdd.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "32b66df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0143cd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 15, 16, 17, 18, 19, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5c8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7d1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession.builder.appName(\"column operations\").master('local[2]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1cc8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-EPRO57M:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>column operations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x184c27179d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e5aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[( \n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ad08e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField,ArrayType\n",
    "data=[ \n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    "]\n",
    "        \n",
    "# schema = StructType([\n",
    "#      StructField('name', StructType([\n",
    "#         StructField('firstname', StringType(), True),\n",
    "#         StructField('middlename', StringType(), True),\n",
    "#          StructField('lastname', StringType(), True)\n",
    "#      ])),\n",
    "#      StructField('languages', ArrayType(StringType()), True),\n",
    "#      StructField('state', StringType(), True),\n",
    "#      StructField('gender', StringType(), True)\n",
    "#  ])\n",
    "\n",
    "schema= StructType([\n",
    "    StructField('name',StructType([\n",
    "        StructField('fname',StringType(),True),\n",
    "        StructField('mname',StringType(),True),\n",
    "        StructField('lname',StringType(),True)\n",
    "        ])),\n",
    "    StructField('languages',ArrayType(StringType()),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('gender',StringType(),True)\n",
    "])\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column functions -- where(),filter(),withColumn(),withColumnRenamed(),sort(),groupBy(),partitionBy(),orderBy(),when()&otherwise()\n",
    "# column operations-- contains(),equals(),isin(),like(),rlike(),asc(),desc(),asc_nulls_first(),desc_nulls_first(),asc_nulls_last(),desc_nulls_last()\n",
    "# typeField(),bitwiseAND, bitwsiseOR,bitwiseXOR,startswith(),endswith()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eb12ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "data2=[Row(name='smith',prop=Row(city='NY', country='usa'),gender='m'),\n",
    "      Row(name='ravi',prop=Row(city='LA',country='usa'),gender='m'),\n",
    "      Row(name='kiranmai',prop=Row(city='FL',country='usa'),gender='f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ebe4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.createDataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d042697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|    name|     prop|gender|\n",
      "+--------+---------+------+\n",
      "|   smith|{NY, usa}|     m|\n",
      "|    ravi|{LA, usa}|     m|\n",
      "|kiranmai|{FL, usa}|     f|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afdb678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyderabad\n",
      "cse\n"
     ]
    }
   ],
   "source": [
    "class employee:\n",
    "    def __init__(self,name,department,city):\n",
    "        self.name=name\n",
    "        self._department=department\n",
    "        self._city=city\n",
    "    def print(self):\n",
    "        print(self.name,self.department,self.city)\n",
    "    def city(self):\n",
    "        print(self._city)\n",
    "    def dept(self):\n",
    "        print(self._department)\n",
    "emp=employee('kishore','cse','hyderabad')\n",
    "# emp.print()\n",
    "emp.city()\n",
    "emp.dept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d359769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|    name|     prop|gender|\n",
      "+--------+---------+------+\n",
      "|kiranmai|{FL, usa}|     f|\n",
      "|    ravi|{LA, usa}|     m|\n",
      "|   smith|{NY, usa}|     m|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df2.orderBy(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "26242afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=df.select(col('name'),col('state'),col('gender')).orderBy(col('state'))\\\n",
    ".groupBy(col('state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1a4e42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [state], value: [name: struct<firstname: string, middlename: string ... 1 more field>, state: string ... 1 more field], type: GroupBy]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58701dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e86aca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57098be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e8e7d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').sum('salary').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef86291e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col('department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f1ded744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c4a6cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|department|        avg(bonus)|\n",
      "+----------+------------------+\n",
      "|     Sales|17666.666666666668|\n",
      "|   Finance|           20250.0|\n",
      "| Marketing|           19500.0|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').avg('bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a70601ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-----------+\n",
      "|department|state|avg(bonus)|avg(salary)|\n",
      "+----------+-----+----------+-----------+\n",
      "|     Sales|   CA|   23000.0|    81000.0|\n",
      "|   Finance|   CA|   23500.0|    94500.0|\n",
      "|     Sales|   NY|   15000.0|    88000.0|\n",
      "|   Finance|   NY|   17000.0|    81000.0|\n",
      "| Marketing|   NY|   21000.0|    91000.0|\n",
      "| Marketing|   CA|   18000.0|    80000.0|\n",
      "+----------+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy aggregate functions are min(),max(),avg(),count(),sum(),mean(),count()\n",
    "# aggregate functions min(),max(),avg(),sum(),count(),mean(),count()\n",
    "\n",
    "df.groupBy('department','state').avg('bonus','salary').alias('avg_bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dbf79890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------------------+---------+\n",
      "|department|sum_salary|       avg_salary|         avg_bonus|sum_bonus|\n",
      "+----------+----------+-----------------+------------------+---------+\n",
      "|     Sales|    257000|85666.66666666667|17666.666666666668|    53000|\n",
      "|   Finance|    351000|          87750.0|           20250.0|    81000|\n",
      "| Marketing|    171000|          85500.0|           19500.0|    39000|\n",
      "+----------+----------+-----------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,sum\n",
    "df.groupBy('department').agg(sum('salary').alias('sum_salary'),\\\n",
    "                                    avg('salary').alias('avg_salary'),\\\n",
    "                                    avg('bonus').alias('avg_bonus'),\\\n",
    "                                    sum('bonus').alias('sum_bonus')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b974c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, prop: struct<city:string,country:string>, gender: string]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f71f170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+------+---+-----+\n",
      "|emp_name|   dept|state|salary|age|bonus|\n",
      "+--------+-------+-----+------+---+-----+\n",
      "|   James|  Sales|   NY| 90000| 34|10000|\n",
      "| Michael|  Sales|   NY| 86000| 56|20000|\n",
      "|  Robert|  Sales|   CA| 81000| 30|23000|\n",
      "|   Maria|Finance|   CA| 90000| 24|23000|\n",
      "+--------+-------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "cols=['emp_name','dept','state','salary','age','bonus']\n",
    "emp1=spark.createDataFrame(data=simpleData,schema=cols)\n",
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9c654e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+------+---+-----+\n",
      "|emp_name|   dept|state|salary|age|bonus|\n",
      "+--------+-------+-----+------+---+-----+\n",
      "|   James|  Sales|   NY| 90000| 34|10000|\n",
      "| Michael|  Sales|   NY| 86000| 56|20000|\n",
      "|  Robert|  Sales|   CA| 81000| 30|23000|\n",
      "|   Maria|Finance|   CA| 90000| 24|23000|\n",
      "+--------+-------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "emp2=spark.createDataFrame(data=simpleData,schema=cols)\n",
    "emp2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8d82b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+------+---+-----+\n",
      "|emp_name|   dept|state|salary|age|bonus|\n",
      "+--------+-------+-----+------+---+-----+\n",
      "|   James|  Sales|   NY| 90000| 34|10000|\n",
      "| Michael|  Sales|   NY| 86000| 56|20000|\n",
      "|  Robert|  Sales|   CA| 81000| 30|23000|\n",
      "|   Maria|Finance|   CA| 90000| 24|23000|\n",
      "|   James|  Sales|   NY| 90000| 34|10000|\n",
      "| Michael|  Sales|   NY| 86000| 56|20000|\n",
      "|  Robert|  Sales|   CA| 81000| 30|23000|\n",
      "|   Maria|Finance|   CA| 90000| 24|23000|\n",
      "+--------+-------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1.union(emp2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9030a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
